# Airflow ETL Pipeline for Historical Weather Data

**Team:**  

- Iuliia Radionova  
- Zoi Theofilakou  
- Eduard Rednic  

---

## Project Overview

This Introduction to Data Engineering group work project focuses on an automated **ETL pipeline** built with **Apache Airflow** for processing historical weather data from Kaggle.  
The pipeline extracts, transforms, validates, and loads data into a structured database and demonstrates Airflow features such as **XCom** and **trigger rules**.

---

## Dataset

**Source:** [Kaggle - Weather History](https://www.kaggle.com/datasets)  
**File:** `weatherHistory.csv.zip`  

---

## ETL Steps

### 1ï¸âƒ£ Extract  

- Downloads the dataset via Kaggle API  
- The ZIP file is being extracted with Python `zipfile`  
- The XCom is used to pass dataset path to next task  

### 2ï¸âƒ£ Transform  

- Cleans and formats the date column  
- Removes the missing values and duplicates  
- Computes the daily & monthly columns 
- Feature engineering includes: precipitation and wind strength  
- Saves the transformed CSVs and passes via XCom  

### 3ï¸âƒ£ Validate  

- Checks the missing values and ranges  
- Detects the outliers and applies trigger rules  
- Continues only when the result is success

### 4ï¸âƒ£ Load  

- Creates an SQLite database
- Loads the daily & monthly data into tables  

### 5ï¸âƒ£ Orchestration (Airflow DAG)  

- Defines all the ETL tasks and dependencies  
- Uses XCom for the task communication  
- Verifies the full DAG execution in Airflow UI  

---

## Team Roles And Contributions

**Eduard Rednic**  

- Set up the GitHub repository and project structure  
- Installed & configured required libraries (Kaggle API, Pandas, SQLite3, Airflow)  
- Implemented the Extract step (Kaggle API, ZIP handling, XCom)  
- Contributed to late Validation
- Created and wrote this README.md

**Zoi Theofilakou** 

- Implemented the Transform step (cleaning, aggregation, feature engineering)  
- Added the daily and monthly averages and precipitation 
- Contributed to the early Validation

**Iuliia Radionova**  

- Built the SQLite database and handled data loading  
- Defined the Airflow DAG structure, dependencies, and trigger rules  
- Managed the XCom connections and verified the DAG execution  

---

## Submission files

- Python script (ETL and Airflow DAG)  
- Database screenshots (daily & monthly tables)  
- Airflow UI screenshot (successful DAG run)  
- Final individual short report (process, issues, solutions, roles)

---

## Tools

Python â€¢ Pandas â€¢ Airflow â€¢ SQLite â€¢ Kaggle API â€¢ Git â€¢ Ubuntu/Linux â€¢ Visual Studio Code

---

## Folder Structure

## ğŸ“‚ Folder Structure

```text
airflow-weather-pipeline
â”œâ”€â”€ dags
â”‚   â””â”€â”€ etl_weather_dags.py
â”‚
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ validate.py
â”‚   â””â”€â”€ load.py
â”‚
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw
â”‚   â””â”€â”€ processed
â”‚
â”œâ”€â”€ database
â”‚   â””â”€â”€ weather_data.db
â”‚
â”œâ”€â”€ screenshots
â”‚
â”œâ”€â”€ reports
â”‚   â”œâ”€â”€ Final_Report_DE_Eduard_Rednic.pdf
â”‚   â”œâ”€â”€ Final_Report_DE_Zoi_Theofilakou.pdf
â”‚   â”œâ”€â”€ Final_Report_DE_Iuliia_Radionova.pdf
â”‚   â””â”€â”€ DE_presentation.pptx
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
